{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from rouge import Rouge\n",
    "import torch\n",
    "import subprocess\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available torch Version 2.3.1\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "Number of CUDA devices: 1\n",
      "CUDA device name: NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA availability\n",
    "print(\"Available torch Version\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Number of CUDA devices:\", torch.cuda.device_count())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA device\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from local disk...\n",
      "Dataset loaded from local disk.\n",
      "Dataset preparing\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Define the path to save the trained model \n",
    "model_save_path = \"saved_model\"\n",
    "\n",
    "# XSum Dataset (Small Subset)\n",
    "dataset_path = \"E:\\\\NIKHIL\\\\ML\\\\Text Summerizer Using Deep Learning\\\\xsum_dataset\"\n",
    "\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "  print(\"Downloading dataset...\")\n",
    "  # Download and save the full dataset \n",
    "  dataset = load_dataset(\"xsum\")\n",
    "  dataset.save_to_disk(dataset_path)\n",
    "  print(\"Dataset downloaded and saved locally.\")\n",
    "else:\n",
    "  print(\"Loading dataset from local disk...\")\n",
    "  dataset = load_from_disk(dataset_path)\n",
    "  print(\"Dataset loaded from local disk.\")\n",
    "\n",
    "print(\"Dataset preparing\")\n",
    "# Create a small subset for evaluation and training\n",
    "train_size = 0.9 # Use a small portion for faster evaluation and training\n",
    "dataset_split = dataset[\"test\"].train_test_split(test_size=1 - train_size, shuffle=True)\n",
    "train_data = dataset_split[\"train\"].select(range(20))\n",
    "eval_data = dataset_split[\"test\"].select(range(10))\n",
    "# print(f\"Train data size: {len(train_data)}\")\n",
    "# print(f\"Evaluation data size: {len(eval_data)}\")\n",
    "print(\"done\")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, data, tokenizer):\n",
    "    self.data = data\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sample = self.data[idx]\n",
    "    cleaned_text = clean_text(sample[\"document\"])\n",
    "    input_ids = tokenize_text(cleaned_text, self.tokenizer).squeeze()\n",
    "    summary = tokenize_text(sample[\"summary\"], self.tokenizer).squeeze()  # Tokenize the summary as well\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"labels\": summary}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    " \n",
    "  text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "  text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "  text = text.lower()  # Convert to lowercase\n",
    "  text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "  return text\n",
    "\n",
    "def tokenize_text(text, tokenizer):\n",
    "  \n",
    "  input_ids = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
    "  return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, data_loader, tokenizer):\n",
    " \n",
    "  rouge = Rouge()\n",
    "  model.eval()  # Set model to evaluation mode\n",
    "\n",
    "  predictions = []\n",
    "  references = []\n",
    "  with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "      input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "\n",
    "      # Generate summary\n",
    "      summary_ids = model.generate(\n",
    "          input_ids=input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True\n",
    "      )\n",
    "\n",
    "      # Decode summaries\n",
    "      for summary_id in summary_ids:\n",
    "        summary = tokenizer.decode(summary_id, skip_special_tokens=True)\n",
    "        predictions.append(summary)\n",
    "\n",
    "      for label in labels:\n",
    "        reference = tokenizer.decode(label, skip_special_tokens=True)\n",
    "        references.append(reference)\n",
    "\n",
    "  # Calculate ROUGE score\n",
    "  rouge_score = rouge.get_scores(predictions, references, avg=True)\n",
    "  print(f\"ROUGE Score: {rouge_score}\")\n",
    "  return rouge_score[\"rouge-l\"][\"f\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_text(text_to_summarize, model, tokenizer):\n",
    " \n",
    "  cleaned_text = clean_text(text_to_summarize)\n",
    "  input_ids = tokenize_text(cleaned_text, tokenizer).to(device)\n",
    "\n",
    "  summary_ids = model.generate(\n",
    "      input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True\n",
    "  )\n",
    "  summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "  return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train_data, tokenizer, epochs=3, batch_size=2, gradient_accumulation_steps=16):\n",
    " \n",
    "  optimizer = Adam(model.parameters(), lr=1e-5)  # Adjust learning rate as needed\n",
    "  model.train()  # Set model to training mode\n",
    "\n",
    "  training_loss = []  # Track training loss for visualization\n",
    "  best_rouge = 0  # Track best ROUGE score for early stopping \n",
    "  patience = 3  # Number of epochs to wait for improvement before stopping \n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Create DataLoader for the training data\n",
    "    train_dataset = CustomDataset(train_data, tokenizer)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for step, batch in enumerate(train_data_loader):\n",
    "      input_ids = batch[\"input_ids\"].to(device)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(input_ids=input_ids, labels=labels)\n",
    "      loss = outputs.loss / gradient_accumulation_steps  # Normalize loss\n",
    "\n",
    "      # Backward pass\n",
    "      loss.backward()\n",
    "\n",
    "      # Gradient accumulation\n",
    "      if (step + 1) % gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "    # Print training loss\n",
    "    print(f\"Epoch: {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "    training_loss.append(epoch_loss)  # Track training loss \n",
    "\n",
    "    # Evaluate model on validation set (optional)\n",
    "    eval_dataset = CustomDataset(eval_data, tokenizer)\n",
    "    eval_data_loader = DataLoader(eval_dataset, batch_size=1)\n",
    "    val_rouge = evaluate_model(model, eval_data_loader, tokenizer)\n",
    "\n",
    "    # Early stopping \n",
    "    if val_rouge > best_rouge:\n",
    "      best_rouge = val_rouge\n",
    "      patience = 3  # Reset patience counter\n",
    "    else:\n",
    "      patience -= 1\n",
    "      if patience == 0:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "  return model  # Return the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:597: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Training Loss: 9.8122\n",
      "ROUGE Score: {'rouge-1': {'r': 0.3960110860931294, 'p': 0.08847719380371691, 'f': 0.14404494360313747}, 'rouge-2': {'r': 0.07212418859090686, 'p': 0.01253075034940521, 'f': 0.02125603274122903}, 'rouge-l': {'r': 0.26453162429246024, 'p': 0.05951817923806291, 'f': 0.09679213322078727}}\n",
      "Epoch: 2/10, Training Loss: 9.8818\n",
      "ROUGE Score: {'rouge-1': {'r': 0.3960110860931294, 'p': 0.08847719380371691, 'f': 0.14404494360313747}, 'rouge-2': {'r': 0.07212418859090686, 'p': 0.01253075034940521, 'f': 0.02125603274122903}, 'rouge-l': {'r': 0.26453162429246024, 'p': 0.05951817923806291, 'f': 0.09679213322078727}}\n",
      "Epoch: 3/10, Training Loss: 9.8818\n",
      "ROUGE Score: {'rouge-1': {'r': 0.3960110860931294, 'p': 0.08847719380371691, 'f': 0.14404494360313747}, 'rouge-2': {'r': 0.07212418859090686, 'p': 0.01253075034940521, 'f': 0.02125603274122903}, 'rouge-l': {'r': 0.26453162429246024, 'p': 0.05951817923806291, 'f': 0.09679213322078727}}\n",
      "Epoch: 4/10, Training Loss: 9.8818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Score: {'rouge-1': {'r': 0.3960110860931294, 'p': 0.08847719380371691, 'f': 0.14404494360313747}, 'rouge-2': {'r': 0.07212418859090686, 'p': 0.01253075034940521, 'f': 0.02125603274122903}, 'rouge-l': {'r': 0.26453162429246024, 'p': 0.05951817923806291, 'f': 0.09679213322078727}}\n",
      "Early stopping triggered!\n",
      "Model saved to saved_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Pre-trained Model Selection\n",
    "model_name = \"facebook/bart-base\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the trained model if it exists, otherwise initialize a new model\n",
    "if os.path.exists(model_save_path):\n",
    "  print(\"Loading trained model from disk...\")\n",
    "  model = AutoModelForSeq2SeqLM.from_pretrained(model_save_path)\n",
    "  # Train the model (adjust epochs, batch size, and gradient accumulation steps for desired training time and memory constraints)\n",
    "  trained_model = train_model(model, train_data, tokenizer, epochs=10, batch_size=2, gradient_accumulation_steps=16)\n",
    "\n",
    "else:\n",
    "  print(\"Loading pre-trained model...\")\n",
    "  model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "  model.to(device)  # Move model to appropriate device\n",
    "\n",
    "  # Train the model (adjust epochs, batch size, and gradient accumulation steps for desired training time and memory constraints)\n",
    "  trained_model = train_model(model, train_data, tokenizer, epochs=10, batch_size=2, gradient_accumulation_steps=16)\n",
    "\n",
    "  # Save the trained model\n",
    "  trained_model.save_pretrained(model_save_path)\n",
    "  print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "model = trained_model\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Score: {'rouge-1': {'r': 0.3960110860931294, 'p': 0.08847719380371691, 'f': 0.14404494360313747}, 'rouge-2': {'r': 0.07212418859090686, 'p': 0.01253075034940521, 'f': 0.02125603274122903}, 'rouge-l': {'r': 0.26453162429246024, 'p': 0.05951817923806291, 'f': 0.09679213322078727}}\n",
      "\n",
      "ROUGE Score: 0.09679213322078727\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on the evaluation set\n",
    "eval_dataset = CustomDataset(eval_data, tokenizer)\n",
    "eval_data_loader = DataLoader(eval_dataset, batch_size=2)\n",
    "rouge_score = evaluate_model(model, eval_data_loader, tokenizer)\n",
    "print(f\"\\nROUGE Score: {rouge_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
