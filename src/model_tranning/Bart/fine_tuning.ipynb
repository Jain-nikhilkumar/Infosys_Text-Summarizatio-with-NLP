{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from rouge import Rouge\n",
    "import torch\n",
    "import subprocess\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available torch Version 2.3.1\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "Number of CUDA devices: 1\n",
      "CUDA device name: NVIDIA GeForce GTX 1650 Ti\n",
      "Tensor on GPU: tensor([[0.2538, 0.2948, 0.6243],\n",
      "        [0.0917, 0.1094, 0.3214],\n",
      "        [0.3831, 0.1380, 0.4759]], device='cuda:0')\n",
      "nvidia-smi output:\n",
      "\n",
      "Sun Jun 30 22:44:16 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 522.06       Driver Version: 522.06       CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   58C    P0    16W /  N/A |    660MiB /  4096MiB |     19%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2540    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7136    C+G   ...Device\\asus_framework.exe    N/A      |\n",
      "|    0   N/A  N/A     13016      C   ...min\\miniconda3\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     15444    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     16320    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     16496      C   ...min\\miniconda3\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     16548      C   ...min\\miniconda3\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     17328    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA availability\n",
    "\n",
    "print(\"Available torch Version\",torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Number of CUDA devices:\", torch.cuda.device_count())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA device\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tensor = torch.rand(3, 3).cuda()\n",
    "    print(\"Tensor on GPU:\", tensor)\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "\n",
    "\n",
    "def print_nvidia_smi():\n",
    "    try:\n",
    "        # Run the nvidia-smi command\n",
    "        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        \n",
    "        # Check if the command was successful\n",
    "        if result.returncode == 0:\n",
    "            print(\"nvidia-smi output:\\n\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(f\"nvidia-smi failed with error code {result.returncode}\")\n",
    "            print(result.stderr)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"nvidia-smi command not found. Make sure NVIDIA drivers are installed.\")\n",
    "\n",
    "# Call the function to print nvidia-smi output\n",
    "print_nvidia_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\NIKHIL\\ML\\Text Summerizer Using Deep Learning\\Bart-fine_tuned_model\n"
     ]
    }
   ],
   "source": [
    "# Define paths and hyperparameters\n",
    "model_name = \"facebook/bart-base\"  # Pre-trained model\n",
    "model_save_path =  \"E:\\\\NIKHIL\\\\ML\\\\Text Summerizer Using Deep Learning\\\\Bart-fine_tuned_model\"\n",
    "print(model_save_path)  # Path to save the trained model\n",
    "epochs = 3  # Training epochs \n",
    "batch_size = 2  # Training batch size \n",
    "gradient_accumulation_steps = 16  # Gradient accumulation for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from local disk...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from local disk.\n",
      "Dataset preparing\n"
     ]
    }
   ],
   "source": [
    "# XSum Dataset (Small Subset)\n",
    "dataset_path = \"E:\\\\NIKHIL\\\\ML\\\\Text Summerizer Using Deep Learning\\\\src\\\\xsum_dataset\"\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "  print(\"Downloading dataset...\")\n",
    "  dataset = load_dataset(\"xsum\")\n",
    "  dataset.save_to_disk(dataset_path)\n",
    "  print(\"Dataset downloaded and saved locally.\")\n",
    "else:\n",
    "  print(\"Loading dataset from local disk...\")\n",
    "  dataset = load_from_disk(dataset_path)\n",
    "  print(\"Dataset loaded from local disk.\")\n",
    "\n",
    "print(\"Dataset preparing\")\n",
    "\n",
    "# Create a small subset for evaluation and training \n",
    "train_size = 0.9  # Use a small portion for faster evaluation and training\n",
    "dataset_split = dataset[\"validation\"].train_test_split(test_size=1 - train_size, shuffle=True)\n",
    "train_data = dataset_split[\"train\"].select(range(5))\n",
    "eval_data = dataset_split[\"test\"].select(range(5))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, data, tokenizer):\n",
    "    self.data = data\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sample = self.data[idx]\n",
    "    cleaned_text = clean_text(sample[\"document\"])\n",
    "    input_ids = tokenize_text(cleaned_text, self.tokenizer).squeeze()\n",
    "    summary = tokenize_text(sample[\"summary\"], self.tokenizer).squeeze()  # Tokenize the summary as well if needed\n",
    "    return {\"input_ids\": input_ids, \"labels\": summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "  text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "  text = text.lower()  # Convert to lowercase\n",
    "  text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "  return text\n",
    "\n",
    "def tokenize_text(text, tokenizer):\n",
    "  \"\"\"\n",
    "  Tokenizes text using the provided tokenizer.\n",
    "  \"\"\"\n",
    "  input_ids = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
    "  return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, tokenizer):\n",
    "  rouge = Rouge()\n",
    "  model.eval()  # Set model to evaluation mode\n",
    "\n",
    "  predictions = []\n",
    "  references = []\n",
    "  with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "      input_ids = batch[\"input_ids\"].to(device)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "\n",
    "      # Generate summary\n",
    "      summary_ids = model.generate(\n",
    "          input_ids=input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True\n",
    "      )\n",
    "\n",
    "      # Decode summaries\n",
    "      for summary_id in summary_ids:\n",
    "        summary = tokenizer.decode(summary_id, skip_special_tokens=True)\n",
    "        predictions.append(summary)\n",
    "\n",
    "      for label in labels:\n",
    "        reference = tokenizer.decode(label, skip_special_tokens=True)\n",
    "        references.append(reference)\n",
    "\n",
    "  # Calculate ROUGE score\n",
    "  rouge_score = rouge.get_scores(predictions, references, avg=True)\n",
    "  print(f\"ROUGE Score: {rouge_score}\")\n",
    "  return rouge_score[\"rouge-l\"][\"f\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text_to_summarize, model, tokenizer):\n",
    "  cleaned_text = clean_text(text_to_summarize)\n",
    "  input_ids = tokenize_text(cleaned_text, tokenizer).to(device)\n",
    "\n",
    "  summary_ids = model.generate(\n",
    "      input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True\n",
    "  )\n",
    "  summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "  return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, tokenizer, epochs=3, batch_size=2, gradient_accumulation_steps=16):\n",
    "  optimizer = Adam(model.parameters(), lr=1e-5)  # Adjust learning rate as needed\n",
    "  model.train()  # Set model to training mode\n",
    "\n",
    "  training_loss = []  # Track training loss for visualization (optional)\n",
    "  best_rouge = 0  # Track best ROUGE score for early stopping (optional)\n",
    "  patience = 3  # Number of epochs to wait for improvement before stopping (optional)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Create DataLoader for the training data\n",
    "    train_dataset = CustomDataset(train_data, tokenizer)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for step, batch in enumerate(train_data_loader):\n",
    "      input_ids = batch[\"input_ids\"].to(device)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(input_ids=input_ids, labels=labels)\n",
    "      loss = outputs.loss / gradient_accumulation_steps  # Normalize loss\n",
    "\n",
    "      # Backward pass\n",
    "      loss.backward()\n",
    "\n",
    "      # Gradient accumulation\n",
    "      if (step + 1) % gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "    # Print training loss\n",
    "    print(f\"Epoch: {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "    training_loss.append(epoch_loss)  # Track training loss \n",
    "\n",
    "    # Evaluate model on validation set\n",
    "    eval_dataset = CustomDataset(eval_data, tokenizer)\n",
    "    eval_data_loader = DataLoader(eval_dataset, batch_size=1)\n",
    "    val_rouge = evaluate_model(model, eval_data_loader, tokenizer)\n",
    "  \n",
    "\n",
    "    # Early stopping\n",
    "    if val_rouge > best_rouge:\n",
    "      best_rouge = val_rouge\n",
    "      patience = 3  # Reset patience counter\n",
    "    else:\n",
    "      patience -= 1\n",
    "      if patience == 0:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "  return model  # Return the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model from: facebook/bart-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:603: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3, Training Loss: 2.9462\n",
      "ROUGE Score: {'rouge-1': {'r': 0.3845454545454545, 'p': 0.099366196287821, 'f': 0.15562081869531363}, 'rouge-2': {'r': 0.060497835497835495, 'p': 0.010754658377043322, 'f': 0.018037140417443864}, 'rouge-l': {'r': 0.23666666666666666, 'p': 0.05896891685736079, 'f': 0.09312242642623349}}\n",
      "Epoch: 2/3, Training Loss: 2.9047\n",
      "ROUGE Score: {'rouge-1': {'r': 0.3845454545454545, 'p': 0.099366196287821, 'f': 0.15562081869531363}, 'rouge-2': {'r': 0.060497835497835495, 'p': 0.010754658377043322, 'f': 0.018037140417443864}, 'rouge-l': {'r': 0.23666666666666666, 'p': 0.05896891685736079, 'f': 0.09312242642623349}}\n",
      "Epoch: 3/3, Training Loss: 2.9047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Score: {'rouge-1': {'r': 0.3845454545454545, 'p': 0.099366196287821, 'f': 0.15562081869531363}, 'rouge-2': {'r': 0.060497835497835495, 'p': 0.010754658377043322, 'f': 0.018037140417443864}, 'rouge-l': {'r': 0.23666666666666666, 'p': 0.05896891685736079, 'f': 0.09312242642623349}}\n",
      "Saved trained model to: E:\\NIKHIL\\ML\\Text Summerizer Using Deep Learning\\Bart-fine_tuned_model\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load or create model\n",
    "if os.path.exists(model_save_path):\n",
    "  model = AutoModelForSeq2SeqLM.from_pretrained(model_save_path)\n",
    "  print(\"Loaded pre-trained model from:\", model_save_path)\n",
    "else:\n",
    "  model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "  print(\"Created new model from:\", model_name)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, train_data, tokenizer, epochs=epochs, batch_size=batch_size, gradient_accumulation_steps=gradient_accumulation_steps)\n",
    "\n",
    "# Save the trained model (optional)\n",
    "if not os.path.exists(model_save_path):\n",
    "  model.save_pretrained(model_save_path)\n",
    "  print(\"Saved trained model to:\", model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Score: {'rouge-1': {'r': 0.3845454545454545, 'p': 0.099366196287821, 'f': 0.15562081869531363}, 'rouge-2': {'r': 0.060497835497835495, 'p': 0.010754658377043322, 'f': 0.018037140417443864}, 'rouge-l': {'r': 0.23666666666666666, 'p': 0.05896891685736079, 'f': 0.09312242642623349}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09312242642623349"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model on validation set\n",
    "eval_dataset = CustomDataset(eval_data, tokenizer)\n",
    "eval_data_loader = DataLoader(eval_dataset, batch_size=1)\n",
    "evaluate_model(model, eval_data_loader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text\n",
      "Article: The Rise of Citizen Science\n",
      "Citizen science, the involvement of the public in scientific research, is rapidly transforming how we understand the world around us.  Traditionally, scientific inquiry has been the domain of professional researchers working in labs and universities. However, citizen science projects are harnessing the power of the public to collect and analyze massive amounts of data, leading to groundbreaking discoveries across various fields.\n",
      "One of the most prominent examples of citizen science is Galaxy Zoo, an online project where volunteers classify galaxies based on their morphology. This project has not only helped astronomers categorize millions of galaxies but also led to the discovery of new galaxy types. Similarly, eBird, a platform where birdwatchers log their sightings, has provided invaluable data on bird populations and migration patterns, crucial for conservation efforts.\n",
      "Citizen science isn't limited to online projects. Initiatives like the National Audubon Society's Christmas Bird Count, a century-old tradition where volunteers conduct annual bird surveys, have yielded long-term datasets that track bird population trends. Likewise, projects involving water quality monitoring or invasive species tracking empower communities to become active participants in protecting their local environments.\n",
      "The rise of citizen science presents several advantages. It allows scientists to gather data at a much larger scale and geographic scope than ever before. This can be particularly valuable in studying phenomena like climate change or species distribution that require global monitoring efforts. Additionally, citizen science fosters public engagement with science, promoting scientific literacy and empowering communities to take ownership of their environment.\n",
      "However, citizen science also faces challenges. Data quality can be a concern, as volunteers may lack the expertise of professional researchers. Project design and training for volunteers are crucial to ensure data accuracy. Additionally, ensuring equitable access to citizen science opportunities is essential to avoid biases in data collection.\n",
      "Despite these challenges, the future of citizen science is bright. Technological advancements like smartphones with built-in sensors and user-friendly online platforms are making participation even easier. As citizen science continues to evolve, it has the potential to revolutionize scientific research and empower communities to become active stewards of our planet.\n",
      "\n",
      "Summary: article the rise of citizen science citizen science the involvement of the public in scientific research is rapidly transforming how we understand the world around us traditionally scientific inquiry has been the domain of professional researchers working in labs and universities however citizen science projects are harnessing the power of the community to collect and analyze massive amounts of data leading to groundbreaking discoveries across various fields one of the most prominent examples of citizen scientists is galaxy zoo an online project where volunteers classify galaxies based on their morphology this project has not only helped astronomers categorize millions of galaxies but also led to the discovery of new galaxy types similarly ebird a platform where birdwatchers log their sightings has provided invaluable data on bird populations and migration patterns crucial for conservation efforts citizen science isnt limited to online projects\n"
     ]
    }
   ],
   "source": [
    "# Summarize new text (optional)\n",
    "new_text = \"\"\"Article: The Rise of Citizen Science\n",
    "Citizen science, the involvement of the public in scientific research, is rapidly transforming how we understand the world around us.  Traditionally, scientific inquiry has been the domain of professional researchers working in labs and universities. However, citizen science projects are harnessing the power of the public to collect and analyze massive amounts of data, leading to groundbreaking discoveries across various fields.\n",
    "One of the most prominent examples of citizen science is Galaxy Zoo, an online project where volunteers classify galaxies based on their morphology. This project has not only helped astronomers categorize millions of galaxies but also led to the discovery of new galaxy types. Similarly, eBird, a platform where birdwatchers log their sightings, has provided invaluable data on bird populations and migration patterns, crucial for conservation efforts.\n",
    "Citizen science isn't limited to online projects. Initiatives like the National Audubon Society's Christmas Bird Count, a century-old tradition where volunteers conduct annual bird surveys, have yielded long-term datasets that track bird population trends. Likewise, projects involving water quality monitoring or invasive species tracking empower communities to become active participants in protecting their local environments.\n",
    "The rise of citizen science presents several advantages. It allows scientists to gather data at a much larger scale and geographic scope than ever before. This can be particularly valuable in studying phenomena like climate change or species distribution that require global monitoring efforts. Additionally, citizen science fosters public engagement with science, promoting scientific literacy and empowering communities to take ownership of their environment.\n",
    "However, citizen science also faces challenges. Data quality can be a concern, as volunteers may lack the expertise of professional researchers. Project design and training for volunteers are crucial to ensure data accuracy. Additionally, ensuring equitable access to citizen science opportunities is essential to avoid biases in data collection.\n",
    "Despite these challenges, the future of citizen science is bright. Technological advancements like smartphones with built-in sensors and user-friendly online platforms are making participation even easier. As citizen science continues to evolve, it has the potential to revolutionize scientific research and empower communities to become active stewards of our planet.\n",
    "\"\"\"\n",
    "print(\"Original Text\")\n",
    "print(new_text)\n",
    "summary = summarize_text(new_text, trained_model, tokenizer)\n",
    "print(\"Summary:\", summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
