{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["Loading trained model...\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_rouge1': 0.47735988149781255, 'eval_rouge2': 0.16400201393707428, 'eval_rougeL': 0.41887281847872987, 'eval_sacrebleu': 3.445056794681839, 'eval_loss': 10.252685546875, 'eval_runtime': 7.096, 'eval_samples_per_second': 0.705, 'eval_steps_per_second': 0.705}\n","{'eval_rouge1': 0.47735988149781255, 'eval_rouge2': 0.16400201393707428, 'eval_rougeL': 0.41887281847872987, 'eval_sacrebleu': 3.445056794681839, 'eval_loss': 10.252685546875, 'eval_runtime': 7.096, 'eval_samples_per_second': 0.705, 'eval_steps_per_second': 0.705}\n"]}],"source":["import os\n","from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments\n","from datasets import load_dataset\n","from rouge_score import rouge_scorer\n","from sacrebleu import corpus_bleu\n","\n","# Paths to the pre-trained model and original fine-tuned model directory\n","pretrained_model_name = \"google/pegasus-xsum\"\n","trained_model_dir = r\"E:\\NIKHIL\\ML\\Text Summerizer Using Deep Learning\\models\\pegasus-fine_tuned_model\"\n","# Path to save the new fine-tuned model (not needed for evaluation)\n","new_trained_model_dir = r\"/content/drive/MyDrive/pegasus-new_fine_tuned_model\"  # Specify new path for saving\n","\n","# Function to check if the trained model exists\n","def model_exists(model_dir):\n","    return os.path.exists(model_dir) and os.path.isdir(model_dir)\n","\n","# Load the tokenizer\n","tokenizer = PegasusTokenizer.from_pretrained(pretrained_model_name)\n","\n","# Load the appropriate model\n","if model_exists(trained_model_dir):\n","    print(\"Loading trained model...\")\n","    model = PegasusForConditionalGeneration.from_pretrained(trained_model_dir)\n","else:\n","    print(\"Loading pre-trained model...\")\n","    model = PegasusForConditionalGeneration.from_pretrained(pretrained_model_name)\n","\n","# Load the CNN/Daily Mail dataset\n","dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n","\n","# Select a small portion of the dataset for evaluation\n","eval_dataset = dataset[\"validation\"].select(range(5))\n","\n","def preprocess_function(examples):\n","    inputs = tokenizer(examples[\"article\"], truncation=True, padding=\"max_length\", max_length=512)\n","    targets = tokenizer(examples[\"highlights\"], truncation=True, padding=\"max_length\", max_length=128)\n","    inputs[\"labels\"] = targets[\"input_ids\"]\n","    return inputs\n","\n","# Apply the preprocessing function to the evaluation dataset\n","eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n","\n","# Remove columns not needed for evaluation\n","eval_dataset = eval_dataset.remove_columns([\"article\", \"highlights\", \"id\"])\n","\n","def compute_metrics(pred):\n","    \"\"\"Calculates ROUGE and SacreBLEU scores.\"\"\"\n","    predictions, labels = pred.predictions, pred.label_ids\n","    \n","    if isinstance(predictions, tuple):\n","        predictions = predictions[0]\n","    \n","    pred_ids = predictions.argmax(-1)  # Get the predicted token IDs\n","\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    labels_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    rouge_scores = [rouge.score(l, p) for l, p in zip(labels_str, pred_str)]\n","\n","    bleu = corpus_bleu(pred_str, [labels_str])\n","\n","    avg_rouge = {\n","        'rouge1': sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n","        'rouge2': sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n","        'rougeL': sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n","    }\n","\n","    # Logging scalar values only\n","    return {\"eval_rouge1\": avg_rouge['rouge1'], \"eval_rouge2\": avg_rouge['rouge2'], \"eval_rougeL\": avg_rouge['rougeL'], \"eval_sacrebleu\": bleu.score}\n","\n","# Define the training arguments (for evaluation purposes)\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    per_device_eval_batch_size=1,  # Smaller batch size for lower power usage\n","    logging_dir=\"./logs\",\n","    logging_steps=50,  # Reduce logging frequency\n","    evaluation_strategy=\"steps\",\n","    eval_steps=50,\n","    disable_tqdm=True\n",")\n","\n","# Initialize the Trainer for evaluation\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    eval_dataset=eval_dataset,\n","    compute_metrics=compute_metrics  # Pass your custom `compute_metrics` function\n",")\n","\n","# Evaluate the model\n","results = trainer.evaluate()\n","print(results)  # This will print a dictionary containing various metrics (loss, ROUGE, etc.)\n"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":2}
